{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Report\"\n",
        "author: Ellie Strande, Nathan Nguyen, Nayeli Castro, Liora Mayats Alpay\n",
        "format:\n",
        "    html:\n",
        "        code-background: true\n",
        "        toc: true\n",
        "# engine: knitr\n",
        "# jupyter: python3\n",
        "---"
      ],
      "id": "92498a23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "The goal of our machine learning algorithm was to improve longer-range weather forecasts of temperature and \n",
        "precipitation by blending physics-based forecasts with machine learning. The dataset provided contains meteorological \n",
        "data capable of improving sub-seasonal forecasts for weather and climate conditions, as well as predicting natural\n",
        "disasters in advance. Having improved long-term forecasts is important as it will help communities and industries adapt\n",
        "to the challenges brought on by climate change, including providing people the opportunity to prepare for extreme \n",
        "weather events. Further, the ability to generate long-term climate predictions will be helpful for research involving \n",
        "ecosystems, wildlife, human societies, and economies. \n",
        "\n",
        "Climate change refers to the long-term changes in the Earth's climate system's average temperature and weather \n",
        "patterns. It is caused by human activities, primarily the burning of fossil fuels, which releases greenhouse gases into\n",
        "the atmosphere and traps heat from the sun, leading to global warming. The effects of climate change include rising sea\n",
        "levels, more intense heat waves, increased frequency and severity of extreme weather events, and changes in \n",
        "precipitation patterns. Climate change is also having a significant impact on ecosystems and wildlife, as well as human \n",
        "societies and economies.\n",
        "\n",
        "By implementing a machine learning algorithm that can predict longer-range weather forecasts of temperature and \n",
        "precipitation, it is hoped that the adverse consequences of climate change will be detected and prepared for long in \n",
        "advance. With purely physics-based models dominating current-day weather forecasting, which is primarily sufficient for \n",
        "short-term predictions, there is a limited forecast horizon. This could be consequential when trying to adapt and \n",
        "prepare for adverse conditions brought on by climate change. \n",
        "\n",
        "# Data\n",
        "\n",
        "## About the Dataset\n",
        "\n",
        "The dataset we were provided was quite extensive, featuring over 200 predictor variables. The data was sourced from \n",
        "a variety of locations and covered a wide range of categories, incliding Madden-Julian oscillation, pressure, and \n",
        "evaporation. We utilized variables such as sea level pressure (slp), MJO phase and amplitute, geopotential height at \n",
        "various millibar values, zonal wind, and weighted average forecasts for precipitation form the NMME model. Overall, the \n",
        "majority of predictor variables used in our model from this dataset were physics-based climate statistics measured in \n",
        "the past. These variables provided a comprehensive and diverse range of information to inform our model development. \n",
        "\n",
        "#### Target Variable: contest-tmp2m-14d__tmp2m\n",
        "\n",
        "The dataset we were working with was very large, which presented various challenges, especially when developing our \n",
        "model. Due to the high number of rows and predictor variables, the process was quite time-consuming and computationally \n",
        "intensive. Despite this, our group was able to find solutions that made the model development process more efficient. \n",
        "We decided to utilize XGBoost for our model as it is an efficient algorithm that is better suited to handle \n",
        "high-dimensional data. Using XGBoost allowed us to mitigate some of the computational challenges presented by the high \n",
        "dimensioanlity of the dataset. \n",
        "\n",
        "## Data Cleaning\n",
        "\n",
        "To begin our analysis, we initiated a thorough data cleaning process. We began by eliminating any missing values by \n",
        "using the dropna() function. Next, we removed any categorical predictor variables that we would not be using in the \n",
        "model. Finally, we employed the technique of Principal Component Analysis (PCA) to optimize the performance fo the \n",
        "model by reducing the number of variables while preserving important information. We will be delving deeper into the \n",
        "specifics of this technique and its application in the modeling section. \n"
      ],
      "id": "cafbe72c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval : false\n",
        "#Drop missing values \n",
        "trainData = trainData.dropna()\n",
        "#Find which variables are strings/continuous (objects)\n",
        "typesDF = pd.DataFrame(trainData.dtypes)\n",
        "typesDF\n",
        "#Create list of predictor variables\n",
        "preds = list(trainData)\n",
        "preds.remove('index')\n",
        "preds.remove('lat')\n",
        "preds.remove('startdate')\n",
        "preds.remove('climateregions__climateregion')\n",
        "preds.remove('mjo1d__phase')\n",
        "preds.remove('mei__meirank')\n",
        "preds.remove('mei__nip')\n",
        "preds.remove('contest-tmp2m-14d__tmp2m')\n",
        "X = trainData[preds]\n",
        "y = trainData['contest-tmp2m-14d__tmp2m']"
      ],
      "id": "7ad9f255",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data is linked below:\n",
        "\n",
        "#### train data: https://www.kaggle.com/competitions/widsdatathon2023/data?select=train_data.csv \n",
        "#### test data: https://www.kaggle.com/competitions/widsdatathon2023/data?select=test_data.csv\n",
        "\n",
        "\n",
        "# Modeling\n",
        "\n",
        "To begin the model development process, we applied a Linear Regression model to the dataset. Linear regression models describe \n",
        "the relationship between a dependent variable and one or more independent variables, finding the line of best fit \n",
        "through the data points. In our initial attempt, we attemped to standardize the continuous variables by generating \n",
        "Z-scores. However, this method did not yield the desired results as it generated an unusually high mean squared error. \n",
        "After removing Z-scoring, the model performed substantially better, yielding a significantly lower mean squared error. \n",
        "\n",
        "## Simple Linear Regression Model\n"
      ],
      "id": "81585fb9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval : false\n",
        "#Train test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, \n",
        "                                                        random_state = 5)\n",
        "#Standardize/Z score\n",
        "z = StandardScaler()d\n",
        "X_train[preds] = z.fit_transform(X_train[preds])\n",
        "X_val[preds] = z.transform(X_val[preds])\n",
        "#Create and fit model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "#Predictions\n",
        "y_pred = lr.predict(X_val)\n",
        "#Print Training MSE\n",
        "print('Train MSE: ', mean_squared_error(y_train, lr.predict(X_train)))\n",
        "#Print Testing MSE\n",
        "print('Test MSE: ', mean_squared_error(y_val, y_pred))"
      ],
      "id": "a1b19459",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although our linear regression model performed fairly well, we were not satisfied with the limited room for growth \n",
        "provided by this model. We also wanted to try a model that did not assume linearity, as linear regression models assume \n",
        "a linear relationship between the independent and dependent variables. \n",
        "\n",
        "Next, we tried using Principal Component Analysis (PCA) to reduce the number of predictor variables in our model. With \n",
        "the large datset we were working with being very time-consuming and computationally expensive, we decided to try \n",
        "dimensionality reduction to reduce the number of variables while preserving important information. PCA reduces the \n",
        "dimensionality of large datasets by transforming data into a new set of linearly uncorrelated variables, called \n",
        "principal components. These principal components are combinations of the original variables that capture the most \n",
        "variance in the data. Consequentially, we found that our linear regression mean squared error was significantly higher \n",
        "when using the principal components in the model as opposed to the original variables. \n",
        "\n",
        "## XGBoost\n",
        "\n",
        "With the goal of generating long-term predictions for temperature and precipitation, our group decided to use XGBoost, \n",
        "an open-source software library for gradient boosing on decision trees. XGBoost is good for time series data and the \n",
        "development of long-term models, which immediately made it a strong fit for this situation. Additionally, XGBoost \n",
        "provides the ability to make design decisions by tuning hyperparameters to determine the most optimally fit model. \n",
        "XGBoost is designed to be efficient and scalable, making it very successful in the past for Kaggle competitions. The \n",
        "amount of control XGBoost provides was appealing because we could continue to improve our model until we were satisfied \n",
        "with the results. XGBoost picks up on patterns and regularities in the data by automatically tuning thousands of \n",
        "learnable parameters, which is why the model demonstrates so much success in time-series scenarios.\n",
        "\n",
        "Our XGBoost model with tuned hyperparameters generated a test mean squared error of 0.135 on the validation set, and a \n",
        "train mean squared error of 0.0656. This demonstrates that our model is slightly overfit. \n",
        "\n",
        "## XGBoost code:\n"
      ],
      "id": "70a6fcc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Split the data into seperate training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = random_state = 5)\n",
        "\n",
        "#Define the xgboost regression model\n",
        "model = XGBRegressor(n_estimators = 1000, max_depth = 10, learning_rate = 0.2,\n",
        "                    colsample_bytree = 0.8, gamma = 0.15,\n",
        "                    min_child_weight = 2, reg_alpha = 42, reg_lambda = 0.5805879363282948, eta = 0.3,\n",
        "                    subsample = 0.8, scale_pos_weight = 1, nthread = 4, seed = 27)\n",
        "\n",
        "#Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Training MSE\n",
        "print(mean_squared_error(y_train, model.predict(X_train)))\n",
        "\n",
        "#Testing MSE\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "#PREDICTING ON TEST DATASET\n",
        "finalPreds = model.predict(testData[preds])\n",
        "\n",
        "#Create dataframe to store the results\n",
        "results = pd.DataFrame(finalPreds, columns = ['contest-tmp2m-14d__tmp2m'])\n",
        "\n",
        "results['index'] = testData['index']\n",
        "\n",
        "results.to_csv('/Users/elliestrande/Desktop/WiDS/solution.csv', index = False)"
      ],
      "id": "17834494",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While having so much control over the hyperparameters allows for a lot of success, it can also be a drawback due to the \n",
        "complexity of developing this type of model. With the large climate dataset we were working with, hyperparameter tuning \n",
        "methods such as GridSearch were not helpful as they took an extreme amount of time and often crashed due to a lack of \n",
        "memory space. Without the ability to use GridSearch and similar hyperparameter tuning methods, we were challenged with \n",
        "the task of learning new methods of hyperparameter tuning that better suited XGBoost. \n",
        "\n",
        "## Hyperparameter Tuning\n",
        "\n",
        "The hyperparameters our model has include:\n",
        "- n_estimators \n",
        "- max_depth\n",
        "- learning_rate\n",
        "- colsample_bytree\n",
        "- gamma\n",
        "- min_child_weight\n",
        "- reg_alpha\n",
        "- reg_lambda\n",
        "- eta\n",
        "- subsample\n",
        "- scale_pos_weight\n",
        "- nthread\n",
        "- seed\n",
        "\n",
        "To select the hyperparameters, we initially tried to utilize GridSearchCV and RandomSelectionCV, which are both \n",
        "built-in tools in many machine learning libraries. Both of these methods work by iterating over a predefined set of \n",
        "hyperparameter values and training a model on each combination, in order to find the combination that results in the \n",
        "best performance. Consequentally, in our case, we found that using GridSearchCV and RandomSelectionCV was not \n",
        "successful. When working with large datasets, these methods can become very time-consuming and resource-intensive. This \n",
        "is because each iteration of the algorithm requires training a new model, which can take a significant amount of time \n",
        "and computational resources. As the number of possible hyperparameter combination increases, the number of iterations \n",
        "required also increases, causing the problem to significantly exacerbate when using XGBoost. For our model, the process \n",
        "took an extreamly long time and often ended up crashing. This is likely due to the fact that the dataset was too large \n",
        "for these methods to handle efficiently. As a result, we had to look for alternative methods for selecting the optimal \n",
        "hyperparameters for our model. \n",
        "\n",
        "Given the inefficiency of using GridSearch and RandomSelectionCV on our large dataset, we decided to resort to a more \n",
        "manual approach for selecting the optimal hyperparameters. This involved learning about the meaning and ideal value \n",
        "ranges for each parameter, and manually adjusting them in an attempt to improve model performence. To aid in the \n",
        "process, we employed a technique called Bayesian optimization, which uses a library called Hyperopt to find the best \n",
        "set of hyperparameters for our model. Hyperopt is a Python library that uses a search algorithm to find the \n",
        "hyperparameter values that minimize the loss function, which is a measure of how well the model is performing. By using \n",
        "Bayesian optimization with Hyperopt, we were able to more efficiently and effectively tune the hyperparameters of our \n",
        "model. \n",
        "\n",
        "## Baysian Optimization\n"
      ],
      "id": "67a13649"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Initialize domain space for range of values\n",
        "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
        "        'gamma': hp.uniform ('gamma', 1,9),\n",
        "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
        "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
        "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
        "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
        "        'n_estimators': 180,\n",
        "        'seed': 0,\n",
        "    }\n",
        "\n",
        "#Define objective function\n",
        "def objective(space):\n",
        "    clf=xgb.XGBRegressor(\n",
        "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
        "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
        "                    colsample_bytree=int(space['colsample_bytree']))\n",
        "    \n",
        "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
        "    \n",
        "    clf.fit(X_train, y_train,\n",
        "            eval_set=evaluation, eval_metric=\"rmse\",\n",
        "            early_stopping_rounds=10,verbose=False)\n",
        "    \n",
        "\n",
        "    pred = clf.predict(X_test)\n",
        "    accuracy = r2_score(y_test, pred>0.5)\n",
        "    print (\"SCORE:\", accuracy)\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "#Optimize \n",
        "best_hyperparams = fmin(fn = objective,\n",
        "                        space = space,\n",
        "                        algo = tpe.suggest,\n",
        "                        max_evals = 50,\n",
        "                        trials = trials)\n",
        "\n",
        "print(\"The best hyperparameters are : \" , \"\\n\")\n",
        "print(best_hyperparams)"
      ],
      "id": "6a027bbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improvements\n",
        "\n",
        "One aspect of our model that we would like to further improve is the selection of hyperparameters. XGBoost is a\n",
        "powerful algorithm that offers a lot of flexibility when building and training a model. While the abundance of \n",
        "hyperparameters allows for fine-tuning and optimal performance, it can be challenging to navigate and optimize such a \n",
        "large number of options. Given more time, we would continue to work on tuning the hyperparameters to better suit our \n",
        "dataset and improve the overall performance of the model. \n",
        "\n",
        "\n",
        "# Limitations\n",
        "\n",
        "During the development of our model, we encountered several limitations in the data provided. One major issue was the \n",
        "presence of missing data, which required us to drop a significant number of data points before fitting the model. This \n",
        "resulted in a loss of information that could have potentially improved the accuracy of the model. In the future, we \n",
        "would like to explore alternative methods for dealing with missing data, such as imputation techniques, which would \n",
        "allow us to retain more of the data and potentially improve the performance of our model. \n",
        "\n",
        "Another limitation we encountered during the development of our model was the large number of variables with unclear \n",
        "names. This made it difficult to understand the meaning and relevance of each predictor, which hindered our ability to \n",
        "identify the most important variables for the model. Additionally, the large dataset and our limited knowlege of the \n",
        "specifics of climate change and weather forecasting, coupled with the time constraints we were working under, made it \n",
        "challenging to fully understand the significance of each variable and how they would affect the performance of the \n",
        "model. This could have potentially affected the accuracy of the model. In future projects, we would like to have a \n",
        "better understanding of the data and variables, as well as more time to explore the data in depth to improve the \n",
        "understanding of the data and the model's performance. \n",
        "\n",
        "We also ran into issues when hypertuning our parameters due to the large size of the dataset. It was difficult to run \n",
        "extensive hyperparameter tuning algorithms as the large dataset required a significant amount of computational power \n",
        "and memory. This made it difficult to run multiple tuning algorithms due to their extemely long runtime and the limited \n",
        "amount of available RAM. This limited our ability to find the optimal set of hyperparameters for the model and could \n",
        "have potentially affected the accuracy of our model. If we had more time, we would like to obtain access to more \n",
        "powerful computational resources to allow us to run more extensive hyperparameter tuning and improve the performance of \n",
        "the model.  We also would like to gain more knowlege on how to optimize hyperparameters with such a large dataset.  \n",
        "\n",
        "# Kaggle Submission\n",
        "\n",
        "- We are currently 128 on the leaderboard \n",
        "- Our best MSE is 1.525 \n",
        "- This is with our tuned XGBoost\n",
        "\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "In conclusion, our experience developing an XGBoost model was a valuable learning opportunity that resulted in a highly \n",
        "accurate model. In addition, it provided valuable insights and skills we will carry throughout our data science \n",
        "careers. XGBoost is a very useful tool for predicting long-term outcomes, and it can handle high-dimensional data, \n",
        "making it perform well for these types of competitions. Coming into the competition, our group only had a little prior \n",
        "experience with XGBoost, so we had to adapt to and overcome various challenges that arose throughout the process. For \n",
        "example, because the dataset was extensive and contained over 200 predictor variables, we had to learn new methods of \n",
        "hyperparameter tuning that would support this high dimensionality. Additionally, we gained experience cleaning and \n",
        "working with a disorganized, large dataset. Finally, we gained crucial experience collaborating with a team from \n",
        "different backgrounds and experience levels to achieve a common goal. \n",
        "\n",
        "The model we developed with XGBoost has the potential to make a real-world impact by improving the field of weather \n",
        "forecasts and disaster preparedness. Using XGBoost, our model can produce highly accurate predictions of long-term \n",
        "outcomes, which can be especially useful in forecasting weather patterns. The current forecasting methods, primarily \n",
        "based on physics-based models, can be improved by incorporating insights from our XGBoost model. Predicting natural \n",
        "disasters well in advance is critical in addressing the challenges posed by climate change. Being able to anticipate \n",
        "severe weather events, such as hurricanes, floods, or droughts, allows us to take proactive measures to minimize the \n",
        "damage and loss of life. \n",
        "\n",
        "Additionally, we can better allocate resources and plan for the future by predicting the likelihood of natural \n",
        "disasters. This is especially important as the effects of climate change continue to worsen. Implementing a machine \n",
        "learning model such as ours is an essential step in ensuring the safety and well-being of communities, as well as in \n",
        "addressing the more immense global challenges presented by climate change. "
      ],
      "id": "4a4d4df7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}